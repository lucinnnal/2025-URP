2025 URP

Title : 동작 키포인트를 이용한 발표자 행동 평가!

URP 진행상황 및 아이디어
Youtube로부터 좋은 발표 예시를 담은 영상과 그렇지 않은 데이터들을 수집하였음. 데이터 수집의 경우 크라우드 소싱과 유사한 방식을 통해 좋은 영상과 그렇지 않은 영상을 팀원들의 집단 지성을 이용하여 투표하고 데이터를 선정함. 데이터 수집 과정에서 대부분의 좋지 않은 발표를 담은 영상들은 자세에 대한 문제점이 많았음(예를 들어,  대본을 보며 고개를 숙이거나, 손을 허리 위에 올리는 등 공적인 말하기를 하는 상황에서는 보편적으로 적절 하지 않은 자세 및 태도들이 있다고 판단하였음. 따라서 “자세” 정보가 불량한 발표를 판단하는데에 중요한 단서가 될 수 있을거라 가정하여 openpose와 같은 모델을 이용하여 추출한 자세 키포인트 정보를 이용하기로 했음) 각각의 영상들을 대략 5초짜리 정도의 클립들로 나누어서 영상을 분할 후 openpose를 이용하여 프레임 단위로 자세 키포인트의 (x,y) 좌표들을 추출하여서 json 파일로 정리함. Keypoint 추출 demo 사진은 demo 폴더를 참조. 
Keypoint 좌표 추출 같은 경우 어떤 발표 데이터들은 하반신이 없는 데이터들이어서 모든 데이터에 대해 통일하여 상반신 키 포인트만 추출을 하였음. 이는 대부분의 안 좋은 발표 영상에서 상반신에서 단서가 나타난다고 판단하여서 큰 문제점이 없을 거라고 생각되나, 다리 꼬기와 같은 자세 역시 안좋은 발표 영상으로 판단하는데에 좋은 단서가 될 수 있다는 것을 생각하면 우리의 키포인트 추출 방식에는 한계가 있음. 
현재는 좋은 발표 영상 데이터에 비해 안좋은 발표 영상이 매우 적은 상태임. 이런 안좋은 발표를 하는 경우를 좋은 발표들 데이터들에 대한 일종의 “이상치”로 판단하는 것이 좋을지, 비디오 데이터 자체를 넣어서 공간적인 특징 추출을 이용한 binary classification(좋다/안좋다)을 하는 것이 좋을지 고민…

Survey


코드 참조
1. 키 포인트 추출 및 csv 저장 방법 : “구글 코렙”에서 keypoint_extraction.ipynb 돌리기
2. 저장된 csv를 json파일로 바꾸기 : data.py 파일 내에서 keypoint 저장한 csv파일 경로와 json 파일 output 경로 입력한 후 python -u data.py 터미널 명령 입력
